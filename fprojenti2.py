# -*- coding: utf-8 -*-
"""FprojeNTI2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A_TDo6_i09I69K8rOcghZ-0LsPk9fy_j

##Read & Clean
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

data=pd.read_csv(r'C:\Users\And\Downloads\Early Suicide Prediction.csv')

#الفهم العام للبيانات
print(data.head())
print(data.columns)
print(data.shape)

""""
1/ البيانات تابعه لمنظمه الصحه العالميه
2/ تتحدث عن حالات الانتحار حول العالم
3/ البيانات مجمعه من سنة 1985 الي 2016
4/ التصنيفات كتالي

Age العمر   Gender النوع     Stress Level	 مستوي التوتر     Academic Performance	الاداء الدراسي

Health Condition	 الحاله الصحيه    Relationship Condition	 حالة العلاقات     Family Problem	  هل يوجد مشاكل اسريه

Depression Level	 مستوي الاكتئاب   Anxiety Level	   مستوي القلق   Mental Support	  وجود داعم نفسي

Self Harm Story	  وجود اذاء نفسي سابق      Suicide Attempt	   محاوله الانتحار

5/ العمود  المستنتج لبناء النموذج عليه هو (Suicide Attempt)
"""

#اكتشاف البيانات
#حجم البيانات (1099, 12)
print (data.info()) #Family Problem          406      العمود دا فيه يقارب من %50 قيم فارغه
print("number of unique")
print(data.nunique())#الاعمده التي تحتوي علي تصنفات اقصي عدد من التصنفات داخل عمود فريد هو 4
print(data.isnull().sum())

print("data_duplic")
data_duplic=data[data.duplicated()]
print(data_duplic)
print("numbr of dupliced") # يوجد قيم متكراره ولكن ممكن تكون علشان هي اعراض لنفس الحاجه فا افضل عدم حذفهاااا
print(data_duplic.sum(numeric_only=True))

#الكتشاف القيم الشاذه
print(data.describe())
sns.boxenplot(data=data)
#plt.yscale("log")
#عندنا شخص عمره متعدي 200  دا هيتم حذفه

#التعامل مع المشاكل
#حذف العمود Family Problem  هو مهم  بس فيه تقريبا 47% فارغ ولو مالته بحاجه ثابته هياثر علي المدل
df=data.copy()
drop_or_ceb_df=df.copy()

#drop_or_ceb_df.dropna(inplace=True,axis=1)
print(drop_or_ceb_df.shape)

#او نحافظ عليه
drop_or_ceb_df['Family Problem'] = drop_or_ceb_df['Family Problem'].fillna('Unknown')
print(drop_or_ceb_df.isnull().sum())
#عندنا خيارين ام نفعل كود الدروب او كود الاكمال الي تلقيه مناسب للنموذج اشتغل عليه انا هكمل علي الاكمال

#حذف القيم الشاذه

Q1 = drop_or_ceb_df['Age'].quantile(0.25)
Q3 = drop_or_ceb_df['Age'].quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR


drop_or_ceb_df = data[(drop_or_ceb_df['Age'] >= lower_bound) & (drop_or_ceb_df['Age'] <= upper_bound)]

print(drop_or_ceb_df.describe())
sns.boxenplot(data=drop_or_ceb_df)
plt.title("After removing outlrar ")

print(drop_or_ceb_df.nunique())

#مرحله العرض
sns.countplot(x=drop_or_ceb_df["Suicide Attempt"])
plt.title("Suicide Attempt")
plt.show
#يوجد عدم توازن

#علاقه بين العمر وحالات الانتحار
sns.boxenplot(data=drop_or_ceb_df,x="Suicide Attempt",y="Age")
plt.title("Suicide Attempt vs Age ")

#تاثير النوع علي معدل الانتحار
sns.countplot(x='Gender', hue='Suicide Attempt', data=drop_or_ceb_df)
plt.title('Suicide Attempt by Gender')
plt.show()

drop_or_ceb_df['Self Harm Story'].value_counts().plot(
    kind='pie', autopct='%1.1f%%', figsize=(6,6)
)
plt.title('Self Harm Storyt Distribution')
plt.ylabel('')
plt.show()

#علاقه بن العمر مرات الاكتاب  و هل له علاقه بالتفكير في الانتحار
sns.scatterplot(data=drop_or_ceb_df,x="Depression Level",y="Age", hue="Suicide Attempt")
plt.title("Depression Level vs Age")

"""##One Hot Encoding"""

#الانكودر
#كل التصنفات الفريد اقصي عدد فيها هو 4 فاهشتغل one hot
from sklearn.preprocessing import OneHotEncoder

categorical_cols = ["Anxiety Level",'Depression Level','Health Condition','Academic Performance','Stress Level','Gender', 'Mental Support', 'Relationship Condition', 'Family Problem', 'Self Harm Story']

encoder = OneHotEncoder(sparse_output=False)
encoded_data = encoder.fit_transform(drop_or_ceb_df[categorical_cols])
encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_cols))
data_numeric = drop_or_ceb_df.drop(columns=categorical_cols)
final_df = pd.concat([data_numeric.reset_index(drop=True), encoded_df.reset_index(drop=True)], axis=1)
final_df# دي اخر داتا ست فيها كل التعديلات

#  ملحوظه لو هتشتغل في الموديل علي الاسقطا العمود الي فيه فيم فارغه الي اسمهFamily Problem متنساش تحذفه من الليست الي هتدخل علي الانكودر علشان متعملش مشكله

final_df['Suicide Attempt'].value_counts()

final_df['Suicide Attempt'].replace(['Never Thought', 'Thought', 'Attempted'], [0, 1, 2], inplace=True)

"""##Split"""

X=final_df.drop(columns=['Suicide Attempt'])
y=final_df['Suicide Attempt']

# from sklearn.preprocessing import StandardScaler
# sc_X = StandardScaler()
# X = sc_X.fit_transform(X)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""##Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42,max_depth=10, min_samples_leaf= 10,min_samples_split=15)

dt.fit(X_train, y_train)
y_pred = dt.predict(X_test)

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

"""###Decision Tree After Sampling"""

from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
ros = RandomOverSampler(
    sampling_strategy='minority',random_state=42)
X_resampled, y_resampled = ros.fit_resample(X, y)

rus= RandomUnderSampler(
    sampling_strategy='majority',random_state=42)
X_resampled, y_resampled = rus.fit_resample(X_resampled, y_resampled)

print("Resampled X shape:", X_resampled.shape)
print("Resampled y shape:", y_resampled.shape)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42,max_depth=10, min_samples_leaf= 10,min_samples_split=15)
dt.fit(X_train, y_train)
y_pred = dt.predict(X_test)

"""####Decision Tree After GridSearchCV"""

from sklearn.model_selection import GridSearchCV
param_grid = {
    'max_depth': [5, 10, 15, 20, None],
    'min_samples_split': [2, 5, 10, 15],
    'min_samples_leaf': [1, 2, 4, 10],
}
grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=10, scoring='accuracy')
grid_search.fit(X_train, y_train)
print("Best parameters found: ", grid_search.best_params_)
best_dt = grid_search.best_estimator_
y_pred_best = best_dt.predict(X_test)
print("Best Model Accuracy:", accuracy_score(y_test, y_pred_best))

"""##Random Forest"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(random_state=42, n_estimators=100, max_depth=10, min_samples_leaf=10, min_samples_split=15)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))
print("Random Forest Confusion Matrix:\n", confusion_matrix(y_test, y_pred_rf))
print("Random Forest Classification Report:\n", classification_report(y_test, y_pred_rf))

"""###Random Forest With GridSearchCV"""

from sklearn.model_selection import GridSearchCV
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
}
grid_search = GridSearchCV(rf, param_grid, cv=10, scoring='accuracy')
grid_search.fit(X_train, y_train)
print("Best parameters found: ", grid_search.best_params_)
best_dt = grid_search.best_estimator_
y_pred_best = best_dt.predict(X_test)
print("Best Model Accuracy:", accuracy_score(y_test, y_pred_best))

"""##TEST Decision Tree&Random Forest with Scaling"""

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_resampled_scaled = scaler.fit_transform(X_resampled)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_resampled_scaled, y_resampled, test_size=0.2, random_state=42)
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42,max_depth=10, min_samples_leaf= 10,min_samples_split=15)
dt.fit(X_train, y_train)
y_pred = dt.predict(X_test)
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

rf = RandomForestClassifier(random_state=42, n_estimators=100, max_depth=10, min_samples_leaf=1, min_samples_split=5)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))
print("Random Forest Confusion Matrix:\n", confusion_matrix(y_test, y_pred_rf))

"""##Neural Network"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras import callbacks
model = Sequential(
    [
        Dense(256, activation='relu', input_shape=(X_resampled.shape[1],)),
        Dense(128, activation='relu'),
        Dense(64, activation='relu'),
        Dense(64, activation='tanh'),
        Dense(32, activation='relu'),
        Dense(16, activation='relu'),
        Dense(8, activation='relu'),
        Dense(3, activation='softmax')  # Assuming 3 classes
    ]
)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
early_stopping = callbacks.EarlyStopping(
    min_delta=0.001,
    patience=20,
    restore_best_weights=True
    )

model.fit(X_train, y_train,
        epochs=50,
        batch_size=32,
        validation_data=(X_test, y_test),
        callbacks=[early_stopping]
        )

"""###NN With sampling"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

model.fit(X_train, y_train,
        epochs=50,
        batch_size=32,
        validation_data=(X_test, y_test),
        callbacks=[early_stopping]
        )

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_resampled_scaled, y_resampled, test_size=0.2, random_state=42)

from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout ,BatchNormalization
from tensorflow.keras import callbacks
model2 = Sequential(
    [
        Dense(256, activation='relu', input_shape=(X_resampled.shape[1],)),
        Dropout(0.7),
        BatchNormalization(),
        Dense(128, activation='relu'),
        BatchNormalization(),
        Dense(64, activation='relu'),
        Dense(64, activation='relu'),
        Dense(16, activation='relu'),
        Dense(8, activation='relu'),
        Dense(3, activation='softmax')  # Assuming 3 classes
    ]
)
model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
early_stopping = callbacks.EarlyStopping(
    min_delta=0.001,
    patience=20,
    restore_best_weights=True
    )

model2.fit(X_train, y_train,
        epochs=50,
        batch_size=32,
        validation_data=(X_test, y_test),
        callbacks=[early_stopping]
        )

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout ,BatchNormalization
from tensorflow.keras import callbacks
model2 = Sequential(
    [
        Dense(256, activation='relu', input_shape=(X_resampled.shape[1],)),
        Dropout(0.5),
        BatchNormalization(),
        Dense(128, activation='relu'),
        BatchNormalization(),
        Dense(64, activation='relu'),
        Dense(64, activation='relu'),
        Dense(16, activation='relu'),
        Dense(8, activation='relu'),
        Dense(3, activation='softmax')  # Assuming 3 classes
    ]
)
model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
early_stopping = callbacks.EarlyStopping(
    min_delta=0.001,
    patience=20,
    restore_best_weights=True
    )

model2.fit(X_train, y_train,
        epochs=50,
        batch_size=64,
        validation_data=(X_test, y_test),
        callbacks=[early_stopping]
        )



